{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0468241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import gzip\n",
    "\n",
    "import requests\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "from networkx import resource_allocation_index, jaccard_coefficient, adamic_adar_index, preferential_attachment\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1036ec65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset does not exist, download it.\n",
    "\n",
    "if not os.path.exists(\"twitter_combined.txt\"):\n",
    "    url = \"http://snap.stanford.edu/data/twitter_combined.txt.gz\"\n",
    "    local_filename = \"twitter_combined.txt.gz\"\n",
    "\n",
    "    print(\"Dataset not found!\")\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        r = requests.get(url, stream=True)\n",
    "        total_size = int(r.headers.get('content-length', 0))\n",
    "        downloaded = 0\n",
    "        chunk_size = 1024\n",
    "        for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "            downloaded += len(chunk)\n",
    "            f.write(chunk)\n",
    "            done = int(50 * downloaded / total_size)\n",
    "            progress = str(round(downloaded / 1048576, 2))\n",
    "            total = str(round(total_size / 1048576, 2))\n",
    "            sys.stdout.write(\"\\rDownloading %s: [%s%s%s] %s/%sMB\" % (local_filename, '=' * done, \">\", ' ' * (50 - done), progress, total))\n",
    "            sys.stdout.flush()\n",
    "    print(\"\\nDownloaded! Extracting GZip...\")\n",
    "    with gzip.open(local_filename, 'rb') as gz:\n",
    "        with open(\"twitter_combined.txt\", 'wb') as twt:\n",
    "            shutil.copyfileobj(gz, twt)\n",
    "    print(\"Extracted Successfully! Deleting the GZip...\")\n",
    "    os.remove(local_filename)\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0bbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If output directories do not exist, create them.\n",
    "\n",
    "if not os.path.isdir(\"data\") and not os.path.isdir(\"graphs\"):\n",
    "    print(\"Creating output directories...\")\n",
    "    os.mkdir(\"data\")\n",
    "    os.mkdir(\"graphs\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6dd734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create graph from dataset [dataset taken from: http://snap.stanford.edu/data/ego-Twitter.html]\n",
    "# and write the GraphML file for visual representation of the graph.\n",
    "\n",
    "graph = nx.Graph()\n",
    "print(\"Reading dataset and creating graph...\")\n",
    "with open(\"twitter_combined.txt\", \"rb\") as twt:\n",
    "    graph = nx.read_edgelist(twt)\n",
    "print(\"Number of Nodes:\", graph.number_of_nodes())\n",
    "print(\"Number of Edges:\", graph.number_of_edges())\n",
    "\n",
    "print(\"\\nWriting GraphML file...\")\n",
    "nx.write_graphml(graph, \"graphs/base.graphml\")\n",
    "print(\"Graph saved in graphs/base.graphml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9652e9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downscale the graph to 1% for faster training of the model during demonstration.\n",
    "\n",
    "n = int(0.01 * graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cdb018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'n' number of random estimated edges.\n",
    "\n",
    "print(\"Estimating links between unconnected nodes...\")\n",
    "positive = random.sample(graph.edges(), n)\n",
    "p_sample = nx.Graph()\n",
    "p_sample.add_edges_from(positive, positive=\"True\")\n",
    "print(\"Number of Nodes: \", p_sample.number_of_nodes())\n",
    "print(\"Number of Edges: \", p_sample.number_of_edges())\n",
    "\n",
    "nx.write_edgelist(p_sample, \"data/positive_sample.txt\", data=[\"positive\"])\n",
    "print(\"Data saved in data/positive_sample.txt\")\n",
    "\n",
    "print(\"\\nWriting GraphML file...\")\n",
    "nx.write_graphml(p_sample, \"graphs/positive_sample.graphml\")\n",
    "print(\"Graph saved in graphs/positive_sample.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e199b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out pairs of nodes that are not connected to each other.\n",
    "\n",
    "print(\"Looking for truly unconnected nodes...\")\n",
    "i = 0\n",
    "n_sample = nx.Graph()\n",
    "while i < n:\n",
    "    edge = random.sample(graph.nodes(), 2)\n",
    "    try:\n",
    "        if edge[1] not in graph.neighbors(edge[0]):\n",
    "            n_sample.add_edge(edge[0], edge[1], positive=\"False\")\n",
    "            i += 1\n",
    "    except:\n",
    "        pass\n",
    "negative = n_sample.edges()\n",
    "\n",
    "print(\"Number of Nodes: \", n_sample.number_of_nodes())\n",
    "print(\"Number of Edges: \", n_sample.number_of_edges())\n",
    "\n",
    "nx.write_edgelist(n_sample, \"data/negative_sample.txt\", data=[\"positive\"])\n",
    "print(\"Data saved in data/negative_sample.txt\")\n",
    "\n",
    "print(\"\\nWriting GraphML file...\")\n",
    "nx.write_graphml(n_sample, \"graphs/negative_sample.graphml\")\n",
    "print(\"Graph saved in graphs/negative_sample.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined graph from both the positive and negative graphs.\n",
    "\n",
    "print(\"Combining positive and negative graphs...\")\n",
    "n_sample.add_edges_from(positive, positive=\"True\")\n",
    "print(\"Number of Nodes: \", p_sample.number_of_nodes())\n",
    "print(\"Number of Edges: \", p_sample.number_of_edges())\n",
    "\n",
    "nx.write_edgelist(n_sample, \"data/combined_sample.txt\", data=[\"positive\"])\n",
    "print(\"Data saved in data/combined_sample.txt\")\n",
    "\n",
    "print(\"\\nWriting GraphML file...\")\n",
    "nx.write_graphml(n_sample, \"graphs/combined_sample.graphml\")\n",
    "print(\"Graph saved in graphs/combined_sample.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fb1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training model by removing the positive (assumed) edges from the original graph.\n",
    "\n",
    "print(\"Preparing training model...\")\n",
    "graph.remove_edges_from(positive)\n",
    "print(\"Number of Nodes: \", p_sample.number_of_nodes())\n",
    "print(\"Number of Edges: \", p_sample.number_of_edges())\n",
    "\n",
    "nx.write_edgelist(graph, \"data/training_model.txt\", data=False)\n",
    "print(\"Data saved in data/training_model.txt\")\n",
    "\n",
    "print(\"\\nWriting GraphML file...\")\n",
    "nx.write_graphml(graph, \"graphs/training_model.graphml\")\n",
    "print(\"Graph saved in graphs/training_model.graphml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5918de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_neighbors(G, edges) -> list:\n",
    "    \"\"\"Returns a list containing common neighbors of all the edges provided.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    G : Graph\n",
    "        The Graph in which the `edges` are to be checked for common neighbors.\n",
    "    edges : list\n",
    "        List of edges from the graph `G`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cn_list : list\n",
    "        List of common neighbors of the `edges`.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> CommonNeighbors = common_neighbors(graph, graph.edges())\n",
    "    \"\"\"\n",
    "    cn_list = []\n",
    "    for edge in edges:\n",
    "        x, y = edge[0], edge[1]\n",
    "        n = 0\n",
    "        try:\n",
    "            n1, n2 = G.neighbors(x), G.neighbors(y)\n",
    "            for _ in n1:\n",
    "                if _ in n2:\n",
    "                    n += 1\n",
    "            cn_list.append((x, y, n))\n",
    "        except:\n",
    "            pass\n",
    "    return cn_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea4e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring methods to create the feature set.\n",
    "scoring_methods = [\n",
    "    common_neighbors,\n",
    "    resource_allocation_index,\n",
    "    jaccard_coefficient,\n",
    "    adamic_adar_index,\n",
    "    preferential_attachment\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e6f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the feature set by applying the scoring method.\n",
    "\n",
    "print(\"Constructing feature set...\")\n",
    "data = []\n",
    "label = [\"Label\"] + [\"1\" for _ in positive] + [\"0\" for _ in negative]\n",
    "\n",
    "for method in scoring_methods:\n",
    "    print(\"Using method:\", method.__name__)\n",
    "    prediction = method(graph, positive)\n",
    "\n",
    "    feature = [method.__name__] + [_[2] for _ in prediction]\n",
    "    prediction = method(graph, negative)\n",
    "    feature += [_[2] for _ in prediction]\n",
    "    \n",
    "    data.append(feature)\n",
    "\n",
    "data.append(label)\n",
    "data = [list(_) for _ in zip(*data)] # transposing the data\n",
    "\n",
    "print(\"Writing data to CSV file...\")\n",
    "with open(\"feature_set.csv\", \"w\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    for _ in data:\n",
    "        writer.writerow(_)\n",
    "print(\"Feature set saved as feature_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d429c5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and shuffle the feature set.\n",
    "\n",
    "r = np.loadtxt(open(\"feature_set.csv\", \"rb\"), delimiter=\",\", skiprows=1)\n",
    "l, b = r.shape\n",
    "np.random.shuffle(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7fda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the feature set for training and testing.\n",
    "\n",
    "model_len = int(0.75 * l)\n",
    "\n",
    "X_train = r[0: model_len, 0: b - 1]\n",
    "Y_train = r[0: model_len, b - 1]\n",
    "\n",
    "X_test = r[model_len: l, 0: b - 1]\n",
    "Y_test = r[model_len: l, b - 1]\n",
    "\n",
    "X_train = normalize(X_train, axis=0, norm=\"max\")\n",
    "X_test = normalize(X_test, axis=0, norm=\"max\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0ef555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using the SVM(Support Vector Machine) Classifier.\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(\"Running SVM Classifier (Start time:\", datetime.datetime.strftime(start_time, r\"%H:%M:%S.%f\") + \")\")\n",
    "\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "res = classifier.predict(X_test)\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(Y_test, res) * 100, \"%\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, res)\n",
    "pl.show()\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "total_time = end_time - start_time\n",
    "print(\"End time:\", datetime.datetime.strftime(end_time, r\"%H:%M:%S.%f\"))\n",
    "print(\"Time taken:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd82422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using Logistic Regression.\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(\"Running Linear Classification using Logistic Regression (Start time:\", datetime.datetime.strftime(start_time, r\"%H:%M:%S.%f\") + \")\")\n",
    "\n",
    "classifier = LogisticRegression(random_state=0, multi_class=\"ovr\")\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "res = classifier.predict(X_test)\n",
    "\n",
    "print(\"Linear accuracy:\", accuracy_score(Y_test, res) * 100, \"%\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, res)\n",
    "pl.show()\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "total_time = end_time - start_time\n",
    "print(\"End Time:\", datetime.datetime.strftime(end_time, r\"%H:%M:%S.%f\"))\n",
    "print(\"Time taken:\", total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a15a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training using the MLP(Multi Layer Classification) Classifier.\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "print(\"Running Multi Layer Classification using MLPClassifier (Start time:\", datetime.datetime.strftime(start_time, r\"%H:%M:%S.%f\") + \")\")\n",
    "\n",
    "classifier = MLPClassifier(random_state=1, learning_rate=\"adaptive\")\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "res = classifier.predict(X_test)\n",
    "\n",
    "print(\"Multi Layer Accuracy:\", accuracy_score(Y_test, res) * 100, \"%\")\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(Y_test, res)\n",
    "pl.show()\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "total_time = end_time - start_time\n",
    "print(\"End Time:\", datetime.datetime.strftime(end_time, r\"%H:%M:%S.%f\"))\n",
    "print(\"Time taken:\", total_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
